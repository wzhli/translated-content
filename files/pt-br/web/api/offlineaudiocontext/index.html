---
title: OfflineAudioContext
slug: Web/API/OfflineAudioContext
translation_of: Web/API/OfflineAudioContext
---
<div>{{APIRef("Web Audio API")}}</div>

<div>A interface <code>OfflineAudioContext</code> é uma interface {{domxref("AudioContext")}} que representa um gráfico de processament de áudio construido a partir de conexões entre {{domxref("AudioNode")}}s. Em contraste com o padrão {{domxref("AudioContext")}}, um <code>OfflineAudioContext</code> não processa o áudio para o hardware do dispositivo; Em vez disso, ele gera, o mais rápido possível, e exibe o resultado para um {{domxref("AudioBuffer")}}.</div>

<p>{{InheritanceDiagram}}</p>

<h2 id="Construtor">Construtor</h2>

<dl>
 <dt>{{domxref("OfflineAudioContext.OfflineAudioContext()")}}</dt>
 <dd>Cria uma nova instância <code>OfflineAudioContext</code>.</dd>
</dl>

<h2 id="Propriedades">Propriedades</h2>

<p><em>Também herda propriedades da sua entidade paterna, {{domxref("BaseAudioContext")}}.</em></p>

<dl>
 <dt>{{domxref('OfflineAudioContext.length')}} {{readonlyinline}}</dt>
 <dd>
 <p>Um número inteiro que representa o tamanho do buffer em quadros de amostra.</p>
 </dd>
</dl>

<h3 id="Manipuladores_de_Eventos">Manipuladores de Eventos</h3>

<dl>
 <dt>{{domxref("OfflineAudioContext.oncomplete")}}</dt>
 <dd>É uma chamada {{event("Event_handlers", "event handler")}} quando o processamento é encerrado, é quando o evento {{event("complete")}}  - do tipo {{domxref("OfflineAudioCompletionEvent")}} - é gerado, após a versão baseada em eventos do {{domxref("OfflineAudioContext.startRendering()")}} é usada.</dd>
</dl>

<h2 id="Métodos">Métodos</h2>

<p><em>Também herda métodos da interface paterna, {{domxref("BaseAudioContext")}}.</em></p>

<dl>
 <dt>{{domxref("OfflineAudioContext.resume()")}}</dt>
 <dd>
 <p>Programa uma suspensão da progressão do tempo no contexto de áudio no horário especificado e retorna uma promessa.</p>
 </dd>
 <dt>{{domxref("OfflineAudioContext.suspend()")}}</dt>
 <dd>
 <p>Agende uma suspensão da progressão do tempo no contexto de áudio no horário especificado e retorna uma promessa.</p>
 </dd>
 <dt>{{domxref("OfflineAudioContext.startRendering()")}}</dt>
 <dd>
 <p>Inicia a renderização do áudio, levando em consideração as conexões atuais e as mudanças programadas atuais. Esta página abrange a versão baseada em eventos e a versão baseada em promessas.</p>
 </dd>
</dl>

<h2 id="Exemplo">Exemplo</h2>

<p>Nesse exemplo, declaramos um ambos {{domxref("AudioContext")}} e um <code>OfflineAudioContext</code> objeto. Nós usamos o <code>AudioContext</code> para carregar uma faixa de áudio via XHR ({{domxref("AudioContext.decodeAudioData")}}), então o <code>OfflineAudioContext</code> para renderizar o áudio em um {{domxref("AudioBufferSourceNode")}} e reproduzir a trilha. Depois que o gráfico de áudio off-line estiver configurado, você deve renderizá-lo para {{domxref("AudioBuffer")}} usando {{domxref("OfflineAudioContext.startRendering")}}.</p>

<p>Quando a 'promise' <code>startRendering()</code> é resolvida, a renderização foi concluída e a saída <code>AudioBuffer</code> é retornada fora da 'promise.</p>

<p>Neste ponto, criamos outro contexto de áudio, criamos um {{domxref("AudioBufferSourceNode")}} dentro dele e configuramos o buffer para ser igual à promessa <code>AudioBuffer</code>. Isso é jogado como parte de um gráfico de áudio padrão simples.</p>

<div class="note">
<p><strong>Nota</strong>: Para um exemplo de trabalho, veja nosso <a href="https://mdn.github.io/webaudio-examples/offline-audio-context-promise/">offline-audio-context-promise</a> Github repo (veja o <a href="https://github.com/mdn/webaudio-examples/tree/master/offline-audio-context-promise">código fonte</a> também.)</p>
</div>

<pre class="brush: js">// define o contexto de áudio online e offline

var audioCtx = new AudioContext();
var offlineCtx = new OfflineAudioContext(2,44100*40,44100);

source = offlineCtx.createBufferSource();

// usa XHR para carregar uma faixa de áudio, e
// decodeAudioData para decodificar e OfflineAudioContext para renderizar

function getData() {
  request = new XMLHttpRequest();

  request.open('GET', 'viper.ogg', true);

  request.responseType = 'arraybuffer';

  request.onload = function() {
    var audioData = request.response;

    audioCtx.decodeAudioData(audioData, function(buffer) {
      myBuffer = buffer;
      source.buffer = myBuffer;
      source.connect(offlineCtx.destination);
      source.start();
      //source.loop = true;
      offlineCtx.startRendering().then(function(renderedBuffer) {
        console.log('Rendering completed successfully');
        var audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        var song = audioCtx.createBufferSource();
        song.buffer = renderedBuffer;

        song.connect(audioCtx.destination);

        play.onclick = function() {
          song.start();
        }
      }).catch(function(err) {
          console.log('Rendering failed: ' + err);
          // Nota: A promessa deve rejeitar quando o StartRendering é chamado uma segunda vez em um OfflineAudioContext
      });
    });
  }

  request.send();
}

// Run getData to start the process off

getData();</pre>

<h2 id="Especificações">Especificações</h2>

<table class="standard-table">
 <tbody>
  <tr>
   <th scope="col">Specification</th>
   <th scope="col">Status</th>
   <th scope="col">Comment</th>
  </tr>
  <tr>
   <td>{{SpecName('Web Audio API', '#OfflineAudioContext', 'OfflineAudioContext')}}</td>
   <td>{{Spec2('Web Audio API')}}</td>
   <td>Initial definition</td>
  </tr>
 </tbody>
</table>

<h2 id="Browser_compatibility">Compatibilidade com navegadores</h2>

<div>
  <p>{{Compat("api.OfflineAudioContext")}}</p>
</div>

<h2 id="Veja_também">Veja também</h2>

<ul>
 <li><a href="/en-US/docs/Web_Audio_API/Using_Web_Audio_API">Usando a API de áudio da Web</a></li>
</ul>
